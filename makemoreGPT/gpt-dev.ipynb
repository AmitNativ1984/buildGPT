{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODING GPT FROM SCRATCH\n",
    "\n",
    "This Notebook follows [Code GPT From Scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY), by Andrej Karpathy. This is my implementation, with my explanations, to better understand the way transformers work. \n",
    "\n",
    "The goal of this notebook is to implement a ***G***enrative ***P***retrained ***T***ransformer ***(GPT)***, that will generate Shakespeare like text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the Dataset\n",
    "In this projects, we are going to use [Tiny Shakespeare Dataset](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt).\n",
    "\n",
    "To prepare the dataset for processing, \n",
    "\n",
    "1. Download shakespeare data\n",
    "2. Create tokenizer - what are the distinct charerchters we have?\n",
    "3. create the enconder -> charecter to int\n",
    "   create the decoder -> int back to charerechter.\n",
    "4. show the first 1000 tokens in the dataset.\n",
    "5. define block size\n",
    "   define batch size\n",
    "6. set the torch.seed to 1337 torch.seed(1037)\n",
    "7. split to 90%-10% train/val\n",
    "8. randomly sample the data.\n",
    "   Create the batch size\n",
    "   \n",
    "   \n",
    " 1. The initial loss should be that of a uniformal random variables:\n",
    "    loss_i = -ln(1/65) [65 charecters/tokens] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget(https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Build the Tokenizer, Encoder and Decoder\n",
    "Our goal is to have the ability to convert a string of text, into a sequence of integers, and back.\n",
    "To do this, we first need to create a tokenizer, that will map each character to an integer. This is done by going over the entire dataset, and building the vocabulary: the set of all unique characters in the dataset.\n",
    "\n",
    "After that, we will create the encoder - that maps a string of text into a sequence of numbers, and the decoder - that maps a sequence of numbers bak to a string of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('Vocabulary size (num of unique chars):', vocab_size)\n",
    "\n",
    "# create a mapping from character to index and vice versa\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# create the encoder (ch -> idx) and decoder (idx -> ch) functions\n",
    "encode = lambda s: [stoi[c] for c in s] #encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) #decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hi there\"))\n",
    "print (decode(encode(\"hi there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(len(data)*0.9)\n",
    "train_data, val_data = data[:n], data[n:]\n",
    "\n",
    "# lets print the first 200 elements of data. This is what our data looks like.\n",
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feeding Data Into The Network\n",
    "\n",
    "We will never feed the entire dataset into the network. Instead we feed it with a sequence, up to a fixed block size. \n",
    "\n",
    "### 2.1. The Idea of Input `Context`, and `Block_Size` \n",
    "We are going to be using `B-Gram Model`: A very simple model, that predicts the `next 1 word` based on a given sequence. The input sequence is called the `context`. The maximum length `context`us called `block_size`. We want the model predict the next word, given `context` of length `[1..block_size]`. Later on, this will be usefull for the transformer network to predict sequences of any given length up to `block_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "print(train_data[:block_size+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "print(f\"The input tensor is: {x}\")\n",
    "print(f\"The target tensor is: {y}\")\n",
    "print(\"So that...\")\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target =  y[t]\n",
    "    print(f\"For input {context}, the output is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Processing Input Batch\n",
    "\n",
    "When `context` size is `[1,...,block_size]`; `block_size=8` We have `8` different input contexts, with corresponding `8` different output targets. In other words, `8` training examples. To improve training speed, we will be adding the `batch` dimension. We will be randomly sampling `batch_size` samples from the training data, and use them for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else val_data\n",
    "    # Sample batch_size random samples from data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i+1: i+ 1 + block_size] for i in ix]).to(device)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch(split=\"train\")\n",
    "\n",
    "print(\"input batch:\")\n",
    "print(f\"shape: {xb.shape}\")\n",
    "print(xb)\n",
    "\n",
    "print(\"target batch:\")\n",
    "print(f\"shape: {yb.shape}\")\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 batches. \n",
    "Each batch has 8 different contexts (`1,..,block_size`).\n",
    "There are 8*4=32 different input-target pairs in every batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"For input tensor: {context}, the target is: {target}\")\n",
    "    print (\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the B-Gram Langunage Model\n",
    "\n",
    "As described before, this is a very simple language model. The target is to predict the next 1st word based on a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\" Bigram Language Model.\n",
    "        Args:\n",
    "            vocab_size (int): Number of tokens (size of vocabulary)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directiry reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # B,T,C\n",
    "        # cross_entropy expects the input (logits) to be of shape (B, C, T)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, C),  targets.view(B*T))\n",
    "\n",
    "        return logits ,loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\" Generate new tokens, given current context.\n",
    "            In a simple B-gram model, we just need to look up the logits for the next token.\n",
    "            In more complex models, we are also intrested in the history of the tokens. In other\n",
    "            words, a larger context.\n",
    "\n",
    "            Args:\n",
    "                idx (Tensor): Initial context\n",
    "                max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "            Returns:\n",
    "                Tensor: Generated tokens\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # idx is a tensor of shape (B, T)\n",
    "        idx.to(device)\n",
    "        for i in range(max_new_tokens):\n",
    "            # get the prediction for the current context\n",
    "            logits, loss = self.forward(idx) # logits shape B, T, C\n",
    "            # In B-gram model, focus only on the last time step.\n",
    "            logits = logits[:, -1, :]   \n",
    "            # convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample the next token from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1).to(device)\n",
    "            idx = torch.cat([idx, next_token], dim=1) # shape B, T+1\n",
    "\n",
    "        return idx\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model.to(device)\n",
    "out, loss = model(xb, yb)\n",
    "print(out.shape)\n",
    "print(loss)\n",
    "\n",
    "# We expect the loss to be around -log(1/vocab_size) = -log(1/65) = 4.17\n",
    "print(f\"Expected loss for uniform distribution: {-torch.log(torch.tensor(1/vocab_size))}\")\n",
    "\n",
    "# Example - generate new tokens\n",
    "print(\"\\n\")\n",
    "print(\"Example - generate new tokens\")\n",
    "print(\"(Model is not trained yet, so it will generate random tokens)\")\n",
    "idx = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "new_tokens = model.generate(idx, 100)[0].tolist()\n",
    "\n",
    "# decode the generated tokens\n",
    "gen_txt = decode(new_tokens)\n",
    "print(gen_txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training the B-Gram Model\n",
    "Without training the model, the results are random. The loss is roughly `-ln(1/vocab_size)`. To improve the model, we will train it with `AdamW` optimizer, and implement a simple training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below, is used to estimate the loss. Instead of logging the loss of every epoch - which is very noisy, it will average the loss over `eval_iters` epochs. We shall run this block on both training and validation data. While training, it is important to see that the training and validation loss are decreasing. If the validation loss is increasing, it is a sign of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    # estimate the loss on training and validation sets\n",
    "    out = {}\n",
    "    losses = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters).to(device)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb = get_batch(split)\n",
    "            logits, loss = model(xb, yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training loop\n",
    "num_epochs = 10000\n",
    "batch_size = 32\n",
    "eval_interval = 100\n",
    "\n",
    "model = model.to(device)\n",
    "for step in range(num_epochs):\n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step: {step}, Train Loss: {losses['train']}, Val Loss: {losses['val']}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(split=\"train\")\n",
    "    # forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Loss = {loss.item()}\") # this prints the loss of the last batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Generating Text From Trained B-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "new_tokens = model.generate(idx, 500)[0].tolist()\n",
    "\n",
    "# decode the generated tokens\n",
    "gen_txt = decode(new_tokens)\n",
    "print(gen_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, looking at the result, we can see that the model has learned to generate text that looks like Shakespeare. However, the result is of low quality. This is because model is still quite simple and can be improved. To improve the model, we would like to learn the inter-relations between tokens in the sentence, even if they are far apart. This is where the **transformer** model comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Transformer Model\n",
    "Up until now, using only the `Bigram Language Model`, we could not learn the inter relations between tokens. To predict the next token, we only used the previous token. **The transformer model, on the other hand, can learn the inter-relations between tokens, even if they are far apart.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 The Attention Module - Learning the Relations Between Tokens\n",
    "\n",
    "In a language model, we would like to predict the next token, based only of preivous (context) tokens. This is different from models used for images, where we would like to learn the inter relations between one token, and the entire set of tokens representing the image.\n",
    "\n",
    "The attention module, which is the at the core of the transformer model, is the building block used to learn and represent the inter-relations between tokens. When the attention module is applied to a sequence of tokens coming from the same input sequence, it is called **self-attention**. When the attention module is applied to two different sequences of tokens, it is called **cross-attention**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Preliminaries - Represting the Information of Previous Tokens as Average\n",
    "As a first step, we would like to develop a method to represent the information of previous tokens. The most simple way to do that, would be to average the embeddings of all previous tokens.\n",
    "\n",
    "Lets look a the next toy example:\n",
    "Lets say our `batch` size is `4`, our `block_size` is `8`, and out `vocab_size` is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 #batch size, time, channels\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to create a new tensor, where each element in the T dimension,\n",
    "# is the mean of all elements before it:\n",
    "# x[b,t] = mean_{i<=t} x[b,i]\n",
    "\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 A Mathematical Trick\n",
    "\n",
    "Although the above method outputs the desired averaging, because it uses `for` loops, it is very inefficient. To improve the efficency, we will use matrix multiplication.\n",
    "\n",
    "A multipication of every matrix with a square matrix of size `nxn`, filled with `1`, results in the summation of the rows.\n",
    "\n",
    "So given the matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10, (3,2)).float()\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('b=')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The running sum of the rows of `b` is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a@b\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, because matrix `a` if a full mask of `1`, the result of all the rows in `c` is the same (we sum all rows in `b`).\n",
    "If instead, matrix `a` will be a lower triangular matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(3,3))\n",
    "print(tril)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every element in `c` will be the sum of all previous elements in the same column in `b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c = tril@b\n",
    "print('b=')\n",
    "print(b)\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, becuase we want to average all the preivous elements, we can normalize matrix `a`, so that the sum of each row is `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(3,3))\n",
    "a = tril / torch.sum(tril, dim=1, keepdim=True)\n",
    "print('a=')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, multiplying `a` by `b` will give us the desired result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('b=')\n",
    "print(b)\n",
    "\n",
    "c = a@b\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the original `xbow`, whith size `(B,T,C)`, we would like to do the same trick, with batch multiplcation. \n",
    "So, if we have a tensor of size `(T,T)` and we multiply it by a tensor of size `(B,T,C)`, pytorch will autmalically broadcast the first tensor to size `(B,T,T)`, and multiply it by the second tensor.\n",
    "\n",
    "`(B,T,T) x (B,T,C) = (B,T,C)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei@x\n",
    "\n",
    "print (\"xbow and xbow are the same. For example\")\n",
    "print(\"xbow[0,:3] \")\n",
    "print(xbow[0,:3])\n",
    "print(\"xbow2[0,:3] \")\n",
    "print(xbow2[0,:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Using Softmax\n",
    "When we train self attension module, we would like the weights to be learned. Thats because not every token will be equally important. So instead of using averaging, we are going to use `softmax` normalization.\n",
    "\n",
    "We start by initializing all of our weights to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei2 = torch.zeros(T, T)\n",
    "wei2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to aggerate only past information, we will set all future elements to `-inf`. This will make the softmax of all future elements to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei2 = wei2.masked_fill(tril==0, float('-inf'))\n",
    "wei2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to apply softmax function, and we will get the desired result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei2 = F.softmax(wei2, dim=1)\n",
    "wei2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the new `xbow` will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow3 = wei2@x\n",
    "print(\"Are xbow2 and xbow3 the same: \", torch.allclose(xbow2, xbow3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Building the Self-Attention Block\n",
    "In the above section, we represented the information stored in previous tokens as an average of their embeddings. As seen from `xbow` matrices, this causes the embeddings to have a similar weight. \n",
    "Instead, we need each of the embedding to have a different weight based on the current embedding. For example, if the current embedding is a noun, we would like the weights of the other embeddings to be higher if they are verbs, and lower if they are adjectives.\n",
    "\n",
    "To do this, we propose two more vectors. The `query` vector `Q`, and the `key` vector `K`. \n",
    "Given the current token, the `Q` vector represents `what am I looking for`. The `K` vector represents `what do I have`. \n",
    "Analogous to the noun example, if the current token is a noun, the `Q` vector will have high weights where it expects verbs to be, and low weights where it expects adjectives to be. The `K` vector will have high weights where verbs are, and low weights where adjectives are.\n",
    "\n",
    "`Q` and `K` are learnable vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "k = key(x)  #(B,T,16)\n",
    "q = query(x) #(B,T,16)\n",
    "\n",
    "wei = q@k.transpose(-2, -1) # (B,T,16) @ (B,16,T) --> (B, T, T)\n",
    "\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, because we don't want to aggeragate any information from future tokens, we are going to use the same trick as before - use a lower triangular matrix and set all future elements to `-inf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to do is to multiply the weights with the value `V` of the current token - also a learnable vector - and normalize with the size of the embedding.\n",
    "The normalization is important to keep the variance of a normal distribution to be `1`, so that the weights don't become one hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = nn.Linear(C, head_size, bias=False)\n",
    "v = value(x) #(B,T,16)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = head_size**(-0.5)*wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei@v\n",
    "v[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **So, finally here is the attention module:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    \"\"\"One head of self attention\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, head_size, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        wei= q @ k.transpose(-2, -1) * C**(-0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Integrating the Self-Attention Module into the Bigram Language Model\n",
    "Now, lets add the self-attention module to the bigram model, so that it will learn the inter-relations between tokens. \n",
    "To do this, we first reduce the number of embeddings in the token embedding layer, and instead add a linear layer after the embedding layer. This will allow the model to learn more complex patterns in the data and increase stability.\n",
    "\n",
    "Second, we add a positional encoding to the input embeddings. This is important because it helps the model learn the relations between tokens also based on their position in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelSignleAttention(nn.Module):\n",
    "    \"\"\" Bigram Language Model.\n",
    "        Args:\n",
    "            vocab_size (int): Number of tokens (size of vocabulary)\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, vocab_size, n_embd=24):\n",
    "        super().__init__()\n",
    "        # Each token directiry reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.atten_head = SelfAttentionHead(n_embd, head_size=16, block_size=block_size)\n",
    "        self.head = nn.Linear(head_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,n_embd)\n",
    "        pos_embd = self.pos_embedding_table(torch.arange(T).to(device)) # (T,n_embd)\n",
    "        x = token_embd + pos_embd   # adding positional embedding to token embedding\n",
    "        x = self.atten_head(x) # apply one head of attention\n",
    "        logits = self.head(x) # (B,T,vocab_size)\n",
    "        # cross_entropy expects the input (logits) to be of shape (B, C, T)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, C),  targets.view(B*T))\n",
    "\n",
    "        return logits ,loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\" Generate new tokens, given current context.\n",
    "            In a simple B-gram model, we just need to look up the logits for the next token.\n",
    "            In more complex models, we are also intrested in the history of the tokens. In other\n",
    "            words, a larger context.\n",
    "\n",
    "            Args:\n",
    "                idx (Tensor): Initial context\n",
    "                max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "            Returns:\n",
    "                Tensor: Generated tokens\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # idx is a tensor of shape (B, T)\n",
    "        idx.to(device)\n",
    "        for i in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] # only consider the last block_size tokens\n",
    "            # get the prediction for the current context\n",
    "            logits, loss = self.forward(idx_cond) # logits shape B, T, C\n",
    "            # In B-gram model, focus only on the last time step.\n",
    "            logits = logits[:, -1, :]   \n",
    "            # convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample the next token from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1).to(device)\n",
    "            idx = torch.cat([idx, next_token], dim=1) # shape B, T+1\n",
    "\n",
    "        return idx\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "model = BigramLanguageModelSignleAttention(vocab_size)\n",
    "model.to(device)\n",
    "out, loss = model(xb, yb)\n",
    "print(out.shape)\n",
    "print(loss)\n",
    "\n",
    "# We expect the loss to be around -log(1/vocab_size) = -log(1/65) = 4.17\n",
    "print(f\"Expected loss for uniform distribution: {-torch.log(torch.tensor(1/vocab_size))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model\n",
    "\n",
    "Lets see if the model has improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training loop\n",
    "num_epochs = 10000\n",
    "batch_size = 32\n",
    "eval_interval = 100\n",
    "lr = 1e-3\n",
    "\n",
    "model = BigramLanguageModelSignleAttention(vocab_size)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "for step in range(num_epochs):\n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step: {step}, Train Loss: {losses['train']}, Val Loss: {losses['val']}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(split=\"train\")\n",
    "    # forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "losses = estimate_loss()\n",
    "print(f\"Step: {step}, Train Loss: {losses['train']}, Val Loss: {losses['val']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we got the following loss:\n",
    "\n",
    "`Step: 9900, Train Loss: 2.461491346359253, Val Loss: 2.5023200511932373`\n",
    "\n",
    "After training the model with the self attention block, we get the following loss:\n",
    "\n",
    "`Step: 9900, Train Loss: 2.3991994857788086, Val Loss: 2.40938138961792`\n",
    "\n",
    "So we got some improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Further Improvement: Multi-Head Attention\n",
    "\n",
    "A single attention head can only learn a single type of inter relations between tokens. To learn multiple types of inter-relations, we can use multiple attention heads in parallel. This is called `multi-head attention`.\n",
    "\n",
    "\n",
    "![multi-head-attention](assets/attention_heads.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a multi-head attention model, all we have to do is to create multiple `Q`, `K`, and `V` vectors, and apply the self-attention module to each of them. Each of the single attention heads will be concatenated, and passed through a linear layer to reduce the dimensionality back to the original size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self attention\"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, n_embd, head_size, block_size):\n",
    "        super().__init__()        \n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(n_embd, head_size, block_size) for _ in range(n_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets integrate the multi-head attention into our model.\n",
    "The important thing to note is reduce the number of embeddings according to the number of heads, so once they are concatenated they will have the same size as the original embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelMultiHeadAttention(nn.Module):\n",
    "    \"\"\" Bigram Language Model.\n",
    "        Args:\n",
    "            vocab_size (int): Number of tokens (size of vocabulary)\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, vocab_size, n_embd=32, n_heads=4):\n",
    "        super().__init__()\n",
    "        # Each token directiry reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        head_size = n_embd // n_heads\n",
    "        self.multi_att_head = MultiHeadAttention(n_heads=n_heads, n_embd=n_embd, head_size=head_size, block_size=block_size)\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,n_embd)\n",
    "        pos_embd = self.pos_embedding_table(torch.arange(T).to(device)) # (T,n_embd)\n",
    "        x = token_embd + pos_embd   # adding positional embedding to token embedding\n",
    "        x = self.multi_att_head(x) # apply one head of attention\n",
    "        logits = self.head(x) # (B,T,vocab_size)\n",
    "        # cross_entropy expects the input (logits) to be of shape (B, C, T)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, C),  targets.view(B*T))\n",
    "\n",
    "        return logits ,loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\" Generate new tokens, given current context.\n",
    "            In a simple B-gram model, we just need to look up the logits for the next token.\n",
    "            In more complex models, we are also intrested in the history of the tokens. In other\n",
    "            words, a larger context.\n",
    "\n",
    "            Args:\n",
    "                idx (Tensor): Initial context\n",
    "                max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "            Returns:\n",
    "                Tensor: Generated tokens\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # idx is a tensor of shape (B, T)\n",
    "        idx.to(device)\n",
    "        for i in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] # only consider the last block_size tokens\n",
    "            # get the prediction for the current context\n",
    "            logits, loss = self.forward(idx_cond) # logits shape B, T, C\n",
    "            # In B-gram model, focus only on the last time step.\n",
    "            logits = logits[:, -1, :]   \n",
    "            # convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample the next token from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1).to(device)\n",
    "            idx = torch.cat([idx, next_token], dim=1) # shape B, T+1\n",
    "\n",
    "        return idx\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "model = BigramLanguageModelMultiHeadAttention(vocab_size)\n",
    "model.to(device)\n",
    "out, loss = model(xb, yb)\n",
    "print(out.shape)\n",
    "print(loss)\n",
    "\n",
    "# We expect the loss to be around -log(1/vocab_size) = -log(1/65) = 4.17\n",
    "print(f\"Expected loss for uniform distribution: {-torch.log(torch.tensor(1/vocab_size))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train the new model and see if we have any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training loop\n",
    "num_epochs = 10000\n",
    "batch_size = 32\n",
    "eval_interval = 100\n",
    "lr = 1e-3\n",
    "\n",
    "model = BigramLanguageModelMultiHeadAttention(vocab_size)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "for step in range(num_epochs):\n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step: {step}, Train Loss: {losses['train']}, Val Loss: {losses['val']}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(split=\"train\")\n",
    "    # forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "losses = estimate_loss()\n",
    "print(f\"Step: {step}, Train Loss: {losses['train']}, Val Loss: {losses['val']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we got further improvement:\n",
    "\n",
    "`Step: 9999, Train Loss: 2.1877453327178955, Val Loss: 2.239150047302246`.\n",
    "\n",
    "Lets generate some text and see what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - generate new tokens\n",
    "idx = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "new_tokens = model.generate(idx, 500)[0].tolist()\n",
    "\n",
    "# decode the generated tokens\n",
    "gen_txt = decode(new_tokens)\n",
    "print(gen_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still room for improvement, but the model is already generating text with some correct english words (i.e: `he`, `I`, `that`,`making`, `here`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. The Transformer Model\n",
    "\n",
    "Now, that we have implemented the multi-head attention module, we can build the transformer model. The strength of the transformer model is that it can learn the inter-relations between tokens, even if they are far apart and complex. \n",
    "\n",
    "Lets look at the transformer block, as described in the paper:\n",
    "\n",
    "![transformer-model](assets/transformer_model.png)\n",
    "\n",
    "We are now going to break down the transformer model into its components:\n",
    "\n",
    "#### **Masked Multi-Head Attention**\n",
    "This is the multi-head attention we have implemented before.\n",
    "\n",
    "#### **Layer Normalization**\n",
    "This is a normalization technique similar to batch normalization, but instead of normalizing over the batch dimension, we normalize over the sequence dimension. This is important because we want to keep the inter-relations in the same sequence, and keep the different sequences separate.\n",
    "\n",
    "In pytorch we can use `nn.LayerNorm` to implement this.\n",
    "\n",
    "*Note:* In contrast to the paper, it is now common to use layer normaliation in the input of the multi-head attention module, and not in the output.\n",
    "\n",
    "#### **Skip Connections**\n",
    "As our network gets deeper, the gradients can become very small, and the training can be very difficult. Skip connections are simply the addition of the input to the output of a layer. This allows the gradients to flow directly to the input of the layer, and at the same time also modify the weights of the layer.\n",
    "\n",
    "#### **Feed Forward Network**\n",
    "The feed forward network is a simple linear layer followed by non-linearity. This helps the model learn more complex patterns in the data.\n",
    "\n",
    "#### **Projection Layer**\n",
    "The projection layer is at the end of the transformer block, and is used to project the output of the transformer block back to the original size of the embeddings. It is also learnable.\n",
    "\n",
    "#### **Dropout**\n",
    "Dropout is a regularization technique used to prevent overfitting. It works by randomly setting a fraction of the input to zero. In pytorch we can use `nn.Dropout` to implement this.\n",
    "\n",
    "\n",
    "Putting all of this together we get the transformer block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple linear layer followed by a ReLU activation\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single transformer block\"\"\"\n",
    "    def __init__(self, n_embd, n_heads, block_size):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_heads\n",
    "        self.att = MultiHeadAttention(n_heads, n_embd, head_size, block_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.ln1(x))   # layer norm and skip connection\n",
    "        x = x + self.ffwd(self.ln2(x))  # layer norm and skip connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets build the BigramTransformer model, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=32, n_heads=4, n_blocks=4):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.block_list = nn.ModuleList([TransformerBlock(n_embd, n_heads, block_size) for _ in range(n_blocks)])\n",
    "        self.blocks = nn.Sequential(*self.block_list)\n",
    "        self.ln = nn.LayerNorm(n_embd)  # layer norm before the final head\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_embd = self.token_embedding_table(idx) # (B,T,n_embd)\n",
    "        pos_embd = self.pos_embedding_table(torch.arange(T, device=device)) # (T,n_embd)\n",
    "\n",
    "        x = token_embd + pos_embd   # adding positional embedding to token embedding\n",
    "        x = self.blocks(x) # apply transformer blocks (B,T,n_embd)\n",
    "        x = self.ln(x) # layer norm\n",
    "        logits = self.head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, C),  targets.view(B*T))\n",
    "\n",
    "        return logits ,loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\" Generate new tokens, given current context.\n",
    "            In a simple B-gram model, we just need to look up the logits for the next token.\n",
    "            In more complex models, we are also intrested in the history of the tokens. In other\n",
    "            words, a larger context.\n",
    "\n",
    "            Args:\n",
    "                idx (Tensor): Initial context\n",
    "                max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "            Returns:\n",
    "                Tensor: Generated tokens\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # idx is a tensor of shape (B, T)\n",
    "        idx.to(device)\n",
    "        for i in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] # only consider the last block_size tokens\n",
    "            # get the prediction for the current context\n",
    "            logits, loss = self.forward(idx_cond) # logits shape B, T, C\n",
    "            # In B-gram model, focus only on the last time step.\n",
    "            logits = logits[:, -1, :]   \n",
    "            # convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample the next token from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1).to(device)\n",
    "            idx = torch.cat([idx, next_token], dim=1) # shape B, T+1\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets update the hyperparameters and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "num_epochs = 10000\n",
    "batch_size = 32\n",
    "n_embd = 32\n",
    "eval_interval = 100\n",
    "lr = 1e-3\n",
    "n_heads = 4\n",
    "n_transformer_blocks = 4\n",
    "\n",
    "model = BigramLanguageModelTransformer(vocab_size, n_embd, n_heads, n_transformer_blocks)\n",
    "model = model.to(device)\n",
    "\n",
    "# pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# training loop\n",
    "for step in range(num_epochs):\n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step: {step}, Train Loss: {losses['train']}, Val Loss: {losses['val']}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(split=\"train\")\n",
    "    # forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "losses = estimate_loss()\n",
    "print(f\"Step: {step}, Train Loss: {losses['train']}, Val Loss: {losses['val']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got further improvement, and the loss dropped to:\n",
    "\n",
    "`Step: 9999, Train Loss: 1.8907207250595093, Val Loss: 2.0092897415161133`\n",
    "\n",
    "Lets generate some text and see what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text from the model\n",
    "context = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "new_tokens = model.generate(context, 500)[0].tolist()\n",
    "\n",
    "# decode the generated tokens\n",
    "gen_txt = decode(new_tokens)\n",
    "print(gen_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better. The words look a lot more like english words, and the model has learned to generate text that looks like Shakespeare.\n",
    "This not perfect, but it is a good result for a simple transformer model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
